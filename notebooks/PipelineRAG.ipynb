{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cead78c6",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d856ee",
   "metadata": {},
   "source": [
    "## ¿Qué es RAG?\n",
    "RAG (Retrieval-Augmented Generation) fue introducido por Lewis et al. (2020) en el paper: *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*.\n",
    "Paper: https://arxiv.org/abs/2005.11401\n",
    "\n",
    "**Se basa principalmente**:\n",
    "- Combinar un modelo de recuperación (como BM25, DPR) con un generador como BART o GPT-2.\n",
    "- Esto permite generar respuestas informadas por evidencia textual relevante.\n",
    "\n",
    "## Componentes de RAG:\n",
    "- **Retriever**: busca documentos relevantes dado una query, selecciona top-k documentos.\n",
    "- **Generator**: genera una respuesta condicionada en la query + documentos.\n",
    "- **Fusion**: puede ser por concatenación, promedio de logits, etc.\n",
    "\n",
    "En nuestro caso, simplificamos usando BM25 y GPT-2, sin entrenamiento adicional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc6b66",
   "metadata": {},
   "source": [
    "## Estructura del pipeline en este notebook\n",
    "\n",
    "Se han usado archivos de texto para facilitar la lectura del corpus donde cada linea es un 'documento' adicionalmente archivos como queries y references mas que todo para probar el modelo.\n",
    "\n",
    "- `corpus.txt`: colección de documentos.\n",
    "- `queries.txt`: preguntas a responder.\n",
    "- `references.txt`: respuestas esperadas (para evaluar BLEU).\n",
    "\n",
    "**Módulos**:\n",
    "- `preprocess_index.py`: tokeniza e indexa documentos con BM25.\n",
    "- `retriever.py`: recupera top-k documentos dados una query.\n",
    "- `generator.py`: genera respuesta usando GPT-2.\n",
    "- `evaluator.py`: calcula BLEU para cada respuesta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63eacd",
   "metadata": {},
   "source": [
    "## Indexar y tokenizar documentos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7406ddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "def load_documents(file_path: str) -> List[str]:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "\n",
    "def preprocess_documents(docs: List[str]) -> List[List[str]]:\n",
    "    return [word_tokenize(doc.lower()) for doc in docs]\n",
    "\n",
    "\n",
    "def build_bm25(tokenized_docs: List[List[str]]) -> BM25Okapi:\n",
    "    return BM25Okapi(tokenized_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e27386",
   "metadata": {},
   "source": [
    "Se tokeniza los documentos anteriormente convertido a minúsculas y una limpieza con `strip()`, luego se crea un índice BM25 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cecbcb7",
   "metadata": {},
   "source": [
    "## Recuperar top-k documentos a partir de una query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd2b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, bm25: BM25Okapi, original_docs: List[str], k: int):\n",
    "        self.bm25 = bm25\n",
    "        self.original_docs = original_docs\n",
    "        self.k = k\n",
    "\n",
    "    def retrieve(self, query: str) -> List[str]:\n",
    "        tokenized_query = word_tokenize(query.lower())\n",
    "        return self.bm25.get_top_n(tokenized_query, self.original_docs, n=self.k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1050f8c",
   "metadata": {},
   "source": [
    "Se ha implementado un componente de recuperación de documentos que, dado una consulta, devuelva los top-k documentos más relevantes usando el algoritmo BM25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608511ab",
   "metadata": {},
   "source": [
    "## Generación de respuesta usando GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e3efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "class GPT2Generator:\n",
    "    def __init__(self, max_tokens=50, temperature=0.7, top_p=0.8):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.model.eval()\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "\n",
    "    def generate(self, query: str, docs: list) -> str:\n",
    "        prompt = f\"Context:\\n- \" + \"\\n- \".join(docs) + f\"\\nQuestion: {query}\\nAnswer:\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_tokens,\n",
    "                do_sample=False,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4456643",
   "metadata": {},
   "source": [
    "Sarga el modelo gpt2 y su tokenizer desde Hugging Face (transformers). Se definen hiperparámetros de generación como :\n",
    "\n",
    "- max_tokens: número máximo de tokens a generar.\n",
    "- temperature: controla la aleatoriedad \n",
    "- top_p: top-p sampling \n",
    "\n",
    "Lo que hace es generar un prompt dado por 'Context', 'Question' y 'Answer'. Tokenizando el prompt a 512 tokens, usa `do_sample=False` para obtener la respuesta más probable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c275b",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "391bd6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def compute_bleu(self, references: list, candidates: list) -> float:\n",
    "        tokenized_refs = [[word_tokenize(ref.lower())] for ref in references]\n",
    "        tokenized_cands = [word_tokenize(cand.lower()) for cand in candidates]\n",
    "        chencherry = SmoothingFunction()\n",
    "        return sentence_bleu(\n",
    "            tokenized_refs[0], tokenized_cands[0], smoothing_function=chencherry.method1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bab045",
   "metadata": {},
   "source": [
    "Se calcula el BLEU score entre una respuesta generada por el modelo (candidate) y una respuesta de referencia (reference).\n",
    "\n",
    "Tokeniza ambas respuestas (referencia y generadas por el modelo), usa un smoothing para evitar que el BLEU sea 0 cuando no hay coincidencias de 4-gramas, posteriormente se calcula el BLEU. Tiene limitaciones ya que solo evalua el primer reference con el primer candidate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa4651",
   "metadata": {},
   "source": [
    "## Prueba usando todo esos pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents(\"../src/pipeline_rag/data/corpus.txt\")\n",
    "preprocessed_docs = preprocess_documents(documents)\n",
    "bm25 = build_bm25(preprocessed_docs)\n",
    "\n",
    "# carga querys y references\n",
    "queries = load_documents(\"../src/pipeline_rag/data/queries.txt\")\n",
    "references = load_documents(\"../src/pipeline_rag/data/references.txt\")\n",
    "\n",
    "# diferentes top-k values\n",
    "top_k_values = [1, 2, 3, 5]\n",
    "\n",
    "results = []\n",
    "\n",
    "generator = GPT2Generator(max_tokens=50, temperature=0.7, top_p=0.8)\n",
    "evaluator = Evaluator()\n",
    "\n",
    "for query, reference in zip(queries, references):\n",
    "    print(f\"\\n=== Consulta: {query} ===\")\n",
    "    for k in top_k_values:\n",
    "        print(f\"\\n--- Top-{k} Documentos ---\")\n",
    "        retriever = BM25Retriever(bm25, documents, k)\n",
    "        top_docs = retriever.retrieve(query)\n",
    "\n",
    "        for i, doc in enumerate(top_docs):\n",
    "            print(f\"[{i+1}] {doc}\")\n",
    "\n",
    "        response = generator.generate(query, top_docs)\n",
    "\n",
    "        print(\"\\n--- Respuesta Generada ---\")\n",
    "        print(response)\n",
    "\n",
    "        bleu_score = evaluator.compute_bleu([reference], [response])\n",
    "        print(\"\\n--- Evaluación BLEU ---\")\n",
    "        print(f\"BLEU: {bleu_score:.4f}\")\n",
    "\n",
    "        results.append([query, k, response, reference, bleu_score])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc36e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
