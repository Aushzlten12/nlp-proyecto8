{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cead78c6",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d856ee",
   "metadata": {},
   "source": [
    "## ¿Qué es RAG?\n",
    "RAG (Retrieval-Augmented Generation) fue introducido por Lewis et al. (2020) en el paper: *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*.\n",
    "Paper: https://arxiv.org/abs/2005.11401\n",
    "\n",
    "**Se basa principalmente**:\n",
    "- Combinar un modelo de recuperación (como BM25, DPR) con un generador como BART o GPT-2.\n",
    "- Esto permite generar respuestas informadas por evidencia textual relevante.\n",
    "\n",
    "## Componentes de RAG:\n",
    "- **Retriever**: busca documentos relevantes dado una query, selecciona top-k documentos.\n",
    "- **Generator**: genera una respuesta condicionada en la query + documentos.\n",
    "- **Fusion**: puede ser por concatenación, promedio de logits, etc.\n",
    "\n",
    "En nuestro caso, simplificamos usando BM25 y GPT-2, sin entrenamiento adicional.\n",
    "\n",
    "## Ecuación de RAG**:\n",
    "\n",
    "$$\n",
    "P(y \\mid q) = \\sum_{i=1}^k P(d_i \\mid q)\\; P(y \\mid q, d_i)\n",
    "$$\n",
    "\n",
    "- $P(d_i \\mid q)$: probabilidad de que el documento $d_i$ sea relevante para la consulta, dada por el retriever.  \n",
    "- $P(y \\mid q, d_i)$: probabilidad de generar la secuencia $y$ condicionado en la consulta $q$ y en el documento $d_i$, estimada por el generator.\n",
    "\n",
    "**Explicación**: \n",
    "\n",
    "Primero el retriever asigna un peso a cada documento según su relevancia, luego el generator produce la respuesta considerando cada documento como contexto. Finalmente, RAG combina estas contribuciones en un sumatorio ponderado, permitiendo que la generación esté directamente anclada a la evidencia recuperada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc6b66",
   "metadata": {},
   "source": [
    "## Estructura del pipeline en este notebook\n",
    "\n",
    "Se han usado archivos de texto para facilitar la lectura del corpus donde cada linea es un 'documento' adicionalmente archivos como queries y references mas que todo para probar el modelo.\n",
    "\n",
    "- `corpus.txt`: colección de documentos.\n",
    "- `queries.txt`: preguntas a responder.\n",
    "- `references.txt`: respuestas esperadas (para evaluar BLEU).\n",
    "\n",
    "**Módulos**:\n",
    "- `preprocess_index.py`: tokeniza e indexa documentos con BM25.\n",
    "- `retriever.py`: recupera top-k documentos dados una query.\n",
    "- `generator.py`: genera respuesta usando GPT-2.\n",
    "- `evaluator.py`: calcula BLEU para cada respuesta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63eacd",
   "metadata": {},
   "source": [
    "## Indexar y tokenizar documentos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7406ddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "def load_documents(file_path: str) -> List[str]:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "\n",
    "def preprocess_documents(docs: List[str]) -> List[List[str]]:\n",
    "    return [word_tokenize(doc.lower()) for doc in docs]\n",
    "\n",
    "\n",
    "def build_bm25(tokenized_docs: List[List[str]]) -> BM25Okapi:\n",
    "    return BM25Okapi(tokenized_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e27386",
   "metadata": {},
   "source": [
    "Se tokeniza los documentos anteriormente convertido a minúsculas y una limpieza con `strip()`, luego se crea un índice BM25 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cecbcb7",
   "metadata": {},
   "source": [
    "## Recuperar top-k documentos a partir de una query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd2b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, bm25: BM25Okapi, original_docs: List[str], k: int):\n",
    "        self.bm25 = bm25\n",
    "        self.original_docs = original_docs\n",
    "        self.k = k\n",
    "\n",
    "    def retrieve(self, query: str) -> List[str]:\n",
    "        tokenized_query = word_tokenize(query.lower())\n",
    "        return self.bm25.get_top_n(tokenized_query, self.original_docs, n=self.k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1050f8c",
   "metadata": {},
   "source": [
    "Se ha implementado un componente de recuperación de documentos que, dado una consulta, devuelva los top-k documentos más relevantes usando el algoritmo BM25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608511ab",
   "metadata": {},
   "source": [
    "## Generación de respuesta usando GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e3efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "class GPT2Generator:\n",
    "    def __init__(self, max_tokens=50, temperature=0.7, top_p=0.8):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.model.eval()\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "\n",
    "    def generate(self, query: str, docs: list) -> str:\n",
    "        prompt = f\"Context:\\n- \" + \"\\n- \".join(docs) + f\"\\nQuestion: {query}\\nAnswer:\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_tokens,\n",
    "                do_sample=False,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4456643",
   "metadata": {},
   "source": [
    "Sarga el modelo gpt2 y su tokenizer desde Hugging Face (transformers). Se definen hiperparámetros de generación como :\n",
    "\n",
    "- max_tokens: número máximo de tokens a generar.\n",
    "- temperature: controla la aleatoriedad \n",
    "- top_p: top-p sampling \n",
    "\n",
    "Lo que hace es generar un prompt dado por 'Context', 'Question' y 'Answer'. Tokenizando el prompt a 512 tokens, usa `do_sample=False` para obtener la respuesta más probable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c275b",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "391bd6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def compute_bleu(self, references: list, candidates: list) -> float:\n",
    "        tokenized_refs = [[word_tokenize(ref.lower())] for ref in references]\n",
    "        tokenized_cands = [word_tokenize(cand.lower()) for cand in candidates]\n",
    "        chencherry = SmoothingFunction()\n",
    "        return sentence_bleu(\n",
    "            tokenized_refs[0], tokenized_cands[0], smoothing_function=chencherry.method1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bab045",
   "metadata": {},
   "source": [
    "Se calcula el BLEU score entre una respuesta generada por el modelo (candidate) y una respuesta de referencia (reference).\n",
    "\n",
    "Tokeniza ambas respuestas (referencia y generadas por el modelo), usa un smoothing para evitar que el BLEU sea 0 cuando no hay coincidencias de 4-gramas, posteriormente se calcula el BLEU. Tiene limitaciones ya que solo evalua el primer reference con el primer candidate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa4651",
   "metadata": {},
   "source": [
    "## Prueba usando todo esos pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8b958e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Consulta: What is BM25 and how is it used in NLP? ===\n",
      "\n",
      "--- Top-1 Documentos ---\n",
      "[1] BM25 is an information retrieval algorithm based on term frequency.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Respuesta Generada ---\n",
      "Context:\n",
      "- BM25 is an information retrieval algorithm based on term frequency.\n",
      "Question: What is BM25 and how is it used in NLP?\n",
      "Answer: BM25 is a term retrieval algorithm based on term frequency.\n",
      "Question: What is the difference between BM25 and NLP?\n",
      "Answer: BM25 is a term retrieval algorithm based on term frequency.\n",
      "Question: What is the difference between N\n",
      "\n",
      "--- Evaluación BLEU ---\n",
      "BLEU: 0.0177\n",
      "\n",
      "--- Top-2 Documentos ---\n",
      "[1] BM25 is an information retrieval algorithm based on term frequency.\n",
      "[2] Artificial intelligence is transforming many industries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Respuesta Generada ---\n",
      "Context:\n",
      "- BM25 is an information retrieval algorithm based on term frequency.\n",
      "- Artificial intelligence is transforming many industries.\n",
      "Question: What is BM25 and how is it used in NLP?\n",
      "Answer: BM25 is a tool for the management of information. It is used to manage information in a way that is not possible with traditional information retrieval systems.\n",
      "- BM25 is a tool for the management of information. It is used to manage information in\n",
      "\n",
      "--- Evaluación BLEU ---\n",
      "BLEU: 0.0171\n",
      "\n",
      "--- Top-3 Documentos ---\n",
      "[1] BM25 is an information retrieval algorithm based on term frequency.\n",
      "[2] Artificial intelligence is transforming many industries.\n",
      "[3] Information retrieval is key to search engines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Respuesta Generada ---\n",
      "Context:\n",
      "- BM25 is an information retrieval algorithm based on term frequency.\n",
      "- Artificial intelligence is transforming many industries.\n",
      "- Information retrieval is key to search engines.\n",
      "Question: What is BM25 and how is it used in NLP?\n",
      "Answer: BM25 is a search engine that can be used to search for information. It is a search engine that can be used to search for information. It is a search engine that can be used to search for information. It is a search engine that can\n",
      "\n",
      "--- Evaluación BLEU ---\n",
      "BLEU: 0.0143\n",
      "\n",
      "--- Top-5 Documentos ---\n",
      "[1] BM25 is an information retrieval algorithm based on term frequency.\n",
      "[2] Artificial intelligence is transforming many industries.\n",
      "[3] Information retrieval is key to search engines.\n",
      "[4] Ranking algorithms determine the relevance of a document to a query.\n",
      "[5] Natural language processing enables machines to understand human language.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Respuesta Generada ---\n",
      "Context:\n",
      "- BM25 is an information retrieval algorithm based on term frequency.\n",
      "- Artificial intelligence is transforming many industries.\n",
      "- Information retrieval is key to search engines.\n",
      "- Ranking algorithms determine the relevance of a document to a query.\n",
      "- Natural language processing enables machines to understand human language.\n",
      "Question: What is BM25 and how is it used in NLP?\n",
      "Answer: BM25 is a search engine that can be used to search for keywords in a document.\n",
      "Question: What is the difference between BM25 and NLP?\n",
      "Answer: BM25 is a search engine that can be used to search for keywords in\n",
      "\n",
      "--- Evaluación BLEU ---\n",
      "BLEU: 0.0177\n",
      "\n",
      "=== Consulta: How does GPT-2 generate text? ===\n",
      "\n",
      "--- Top-1 Documentos ---\n",
      "[1] Language models like GPT-2 can autonomously generate coherent text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Respuesta Generada ---\n",
      "Context:\n",
      "- Language models like GPT-2 can autonomously generate coherent text.\n",
      "Question: How does GPT-2 generate text?\n",
      "Answer:\n",
      "- The GPT-2 language model is a set of models that can generate text.\n",
      "- The GPT-2 language model is a set of models that can generate text. - The GPT-2 language model is a set of\n",
      "\n",
      "--- Evaluación BLEU ---\n",
      "BLEU: 0.0042\n",
      "\n",
      "--- Top-2 Documentos ---\n",
      "[1] Language models like GPT-2 can autonomously generate coherent text.\n",
      "[2] GPT-2 was trained on a large corpus of internet text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Respuesta Generada ---\n",
      "Context:\n",
      "- Language models like GPT-2 can autonomously generate coherent text.\n",
      "- GPT-2 was trained on a large corpus of internet text.\n",
      "Question: How does GPT-2 generate text?\n",
      "Answer:\n",
      "- The corpus of internet text is a large corpus of text.\n",
      "- The corpus of internet text is a large corpus of text.\n",
      "- The corpus of internet text is a large corpus of text.\n",
      "- The corpus of internet text is\n",
      "\n",
      "--- Evaluación BLEU ---\n",
      "BLEU: 0.0065\n",
      "\n",
      "--- Top-3 Documentos ---\n",
      "[1] Language models like GPT-2 can autonomously generate coherent text.\n",
      "[2] GPT-2 was trained on a large corpus of internet text.\n",
      "[3] Ranking algorithms determine the relevance of a document to a query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Respuesta Generada ---\n",
      "Context:\n",
      "- Language models like GPT-2 can autonomously generate coherent text.\n",
      "- GPT-2 was trained on a large corpus of internet text.\n",
      "- Ranking algorithms determine the relevance of a document to a query.\n",
      "Question: How does GPT-2 generate text?\n",
      "Answer:\n",
      "- The GPT-2 algorithm generates text based on the following:\n",
      "- The text is a set of words that are related to the text.\n",
      "- The text is a set of words that are related to the text.\n",
      "- The\n",
      "\n",
      "--- Evaluación BLEU ---\n",
      "BLEU: 0.0071\n",
      "\n",
      "--- Top-5 Documentos ---\n",
      "[1] Language models like GPT-2 can autonomously generate coherent text.\n",
      "[2] GPT-2 was trained on a large corpus of internet text.\n",
      "[3] Ranking algorithms determine the relevance of a document to a query.\n",
      "[4] Natural language processing enables machines to understand human language.\n",
      "[5] Information retrieval is key to search engines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Respuesta Generada ---\n",
      "Context:\n",
      "- Language models like GPT-2 can autonomously generate coherent text.\n",
      "- GPT-2 was trained on a large corpus of internet text.\n",
      "- Ranking algorithms determine the relevance of a document to a query.\n",
      "- Natural language processing enables machines to understand human language.\n",
      "- Information retrieval is key to search engines.\n",
      "Question: How does GPT-2 generate text?\n",
      "Answer:\n",
      "- The GPT-2 algorithm generates text based on the following:\n",
      "- The text is generated by a machine learning algorithm.\n",
      "- The text is generated by a human.\n",
      "- The text is generated by a machine learning algorithm.\n",
      "\n",
      "--- Evaluación BLEU ---\n",
      "BLEU: 0.0058\n",
      "\n",
      "=== Consulta: Why is information retrieval important? ===\n",
      "\n",
      "--- Top-1 Documentos ---\n",
      "[1] Information retrieval is key to search engines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Respuesta Generada ---\n",
      "Context:\n",
      "- Information retrieval is key to search engines.\n",
      "Question: Why is information retrieval important?\n",
      "Answer: Information retrieval is important because it allows us to understand the information we are looking for.\n",
      "Question: What is the difference between information retrieval and search engines?\n",
      "Answer: Information retrieval is a way to understand the information we are looking for.\n",
      "Question\n",
      "\n",
      "--- Evaluación BLEU ---\n",
      "BLEU: 0.0063\n",
      "\n",
      "--- Top-2 Documentos ---\n",
      "[1] Information retrieval is key to search engines.\n",
      "[2] BM25 is an information retrieval algorithm based on term frequency.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Respuesta Generada ---\n",
      "Context:\n",
      "- Information retrieval is key to search engines.\n",
      "- BM25 is an information retrieval algorithm based on term frequency.\n",
      "Question: Why is information retrieval important?\n",
      "Answer: Information retrieval is important because it allows us to search for information that is not available to us.\n",
      "Question: What is the difference between information retrieval and search engines?\n",
      "Answer: Information retrieval is a search engine that uses a search engine to search for\n",
      "\n",
      "--- Evaluación BLEU ---\n",
      "BLEU: 0.0071\n",
      "\n",
      "--- Top-3 Documentos ---\n",
      "[1] Information retrieval is key to search engines.\n",
      "[2] BM25 is an information retrieval algorithm based on term frequency.\n",
      "[3] Artificial intelligence is transforming many industries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Respuesta Generada ---\n",
      "Context:\n",
      "- Information retrieval is key to search engines.\n",
      "- BM25 is an information retrieval algorithm based on term frequency.\n",
      "- Artificial intelligence is transforming many industries.\n",
      "Question: Why is information retrieval important?\n",
      "Answer: Information retrieval is key to search engines.\n",
      "- BM25 is an information retrieval algorithm based on term frequency.\n",
      "- Artificial intelligence is transforming many industries.\n",
      "Question: Why is information retrieval important?\n",
      "Answer: Information retrieval is key to search engines\n",
      "\n",
      "--- Evaluación BLEU ---\n",
      "BLEU: 0.0067\n",
      "\n",
      "--- Top-5 Documentos ---\n",
      "[1] Information retrieval is key to search engines.\n",
      "[2] BM25 is an information retrieval algorithm based on term frequency.\n",
      "[3] Artificial intelligence is transforming many industries.\n",
      "[4] Ranking algorithms determine the relevance of a document to a query.\n",
      "[5] Natural language processing enables machines to understand human language.\n",
      "\n",
      "--- Respuesta Generada ---\n",
      "Context:\n",
      "- Information retrieval is key to search engines.\n",
      "- BM25 is an information retrieval algorithm based on term frequency.\n",
      "- Artificial intelligence is transforming many industries.\n",
      "- Ranking algorithms determine the relevance of a document to a query.\n",
      "- Natural language processing enables machines to understand human language.\n",
      "Question: Why is information retrieval important?\n",
      "Answer: Information retrieval is important because it allows us to understand the world around us.\n",
      "- The world is changing rapidly.\n",
      "- The world is changing rapidly.\n",
      "- The world is changing rapidly.\n",
      "- The world is changing rapidly.\n",
      "- The\n",
      "\n",
      "--- Evaluación BLEU ---\n",
      "BLEU: 0.0052\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents(\"../src/pipeline_rag/data/corpus.txt\")\n",
    "preprocessed_docs = preprocess_documents(documents)\n",
    "bm25 = build_bm25(preprocessed_docs)\n",
    "\n",
    "# carga querys y references\n",
    "queries = load_documents(\"../src/pipeline_rag/data/queries.txt\")\n",
    "references = load_documents(\"../src/pipeline_rag/data/references.txt\")\n",
    "\n",
    "# diferentes top-k values\n",
    "top_k_values = [1, 2, 3, 5]\n",
    "\n",
    "results = []\n",
    "\n",
    "generator = GPT2Generator(max_tokens=50, temperature=0.7, top_p=0.8)\n",
    "evaluator = Evaluator()\n",
    "\n",
    "for query, reference in zip(queries, references):\n",
    "    print(f\"\\n=== Consulta: {query} ===\")\n",
    "    for k in top_k_values:\n",
    "        print(f\"\\n--- Top-{k} Documentos ---\")\n",
    "        retriever = BM25Retriever(bm25, documents, k)\n",
    "        top_docs = retriever.retrieve(query)\n",
    "\n",
    "        for i, doc in enumerate(top_docs):\n",
    "            print(f\"[{i+1}] {doc}\")\n",
    "\n",
    "        response = generator.generate(query, top_docs)\n",
    "\n",
    "        print(\"\\n--- Respuesta Generada ---\")\n",
    "        print(response)\n",
    "\n",
    "        bleu_score = evaluator.compute_bleu([reference], [response])\n",
    "        print(\"\\n--- Evaluación BLEU ---\")\n",
    "        print(f\"BLEU: {bleu_score:.4f}\")\n",
    "\n",
    "        results.append([query, k, response, reference, bleu_score])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ddb7d",
   "metadata": {},
   "source": [
    "## Interpretación de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade77050",
   "metadata": {},
   "source": [
    "|Componente|\tResultado\t|Interpretación|\n",
    "|-----------|-----------|--------------|\n",
    "|BM25 Retrieval | Recupera bien los documentos clave\t|Correcto|\n",
    "|GPT-2 Generation\t|Repetitivo, difuso, poco dirigido\t|Limita BLEU|\n",
    "|BLEU Evaluation\t| Puntajes bajos en general\t|Esperado dada la diferencia textual|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a2963",
   "metadata": {},
   "source": [
    "Este experimento muestra que:\n",
    "\n",
    "- BM25 funciona bien como recuperador simple.\n",
    "- GPT-2 no es ideal para tareas de QA sin fine-tuning.\n",
    "- BLEU es útil como métrica, pero muy estricta en casos como este, en la que se usó GTP-2 lo cual no es ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4294e638",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc36e4",
   "metadata": {},
   "source": [
    "- BM25 es efectivo para queries bien representadas en el corpus, pero limitado cuando se requiere interpretación semántica.\n",
    "- GPT-2 es un generador potente, pero no especializado para QA. Se requiere fine-tuning o prompts más controlados para mejorar precisión.\n",
    "- BLEU funciona como métrica formal requerida, pero no refleja bien la calidad semántica de las respuestas generadas por modelos como GPT-2.\n",
    "- Mejora la cobertura semántica, con esto se obtendría mejores `top-k` y se daría material más relevante a GPT-2 para generar mejores respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
